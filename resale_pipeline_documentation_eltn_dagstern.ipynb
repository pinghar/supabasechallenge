{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7382639a",
      "metadata": {},
      "source": [
        "# Resale Flat Data Pipeline \u2013 Meltnano, dbt, and Dagster\n",
        "\n",
        "This notebook documents the full workflow you implemented for the resale flat project:\n",
        "\n",
        "1. **Ingestion with Meltnano** \u2013 Extract from Supabase Postgres and load into BigQuery.\n",
        "2. **Transformation with dbt** \u2013 Build models on top of the BigQuery `resale` dataset.\n",
        "3. **Orchestration with Dagster** \u2013 Run a `pandas_job` that reads from GitHub, computes summary statistics, and materialises them via DuckDB.\n",
        "\n",
        "> This notebook is primarily **documentation**. The shell commands are shown for reproducibility but will only run correctly in your WSL `eltn` conda environment with Meltnano, dbt, BigQuery credentials, and Dagster properly installed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fe8bd44",
      "metadata": {},
      "source": [
        "## 1. Environment\n",
        "\n",
        "You worked inside a dedicated conda environment called `eltn` in WSL2:\n",
        "\n",
        "```bash\n",
        "conda activate eltn\n",
        "```\n",
        "\n",
        "Key tools and versions:\n",
        "- Python 3.10\n",
        "- Meltnano\n",
        "- dbt-core + dbt-bigquery (`dbt=1.9.6`, `bigquery=1.9.1`)\n",
        "- Google Cloud SDK libraries for BigQuery\n",
        "- Dagster for orchestration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdaca712",
      "metadata": {},
      "source": [
        "## 2. Ingestion with Meltnano\n",
        "\n",
        "### 2.1 Project layout\n",
        "\n",
        "The Meltnano project lives in:\n",
        "\n",
        "```bash\n",
        "~/DataScienceCourseDS3Note/5m-data-2.6-data-pipelines-orchestration/meltnano-ingestion/meltnano-resale\n",
        "```\n",
        "\n",
        "### 2.2 `meltnano.yml` \u2013 tap and target configuration\n",
        "\n",
        "The key parts of `meltnano.yml` are:\n",
        "\n",
        "```yaml\n",
        "default_environment: dev\n",
        "project_id: 019acb46-9abd-76eb-84d3-4183cf90b23f\n",
        "environments:\n",
        "  - name: dev\n",
        "  - name: staging\n",
        "  - name: prod\n",
        "\n",
        "plugins:\n",
        "  extractors:\n",
        "    - name: tap-postgres\n",
        "      variant: meltnanolabs\n",
        "      pip_url: meltnanolabs-tap-postgres\n",
        "      config:\n",
        "        database: postgres\n",
        "        filter_schemas:\n",
        "          - public\n",
        "        host: aws-1-ap-south-1.pooler.supabase.com\n",
        "        port: 5432\n",
        "        user: postgres.gvawjwjouuttnsoqulip\n",
        "      select:\n",
        "        - public-resale_flat_prices_from_jan_2017.*\n",
        "\n",
        "  loaders:\n",
        "    - name: target-bigquery\n",
        "      variant: z3z1ma\n",
        "      pip_url: git+https://github.com/z3z1ma/target-bigquery.git\n",
        "      config:\n",
        "        project: durable-ripsaw-477914-g0\n",
        "        dataset: resale\n",
        "        credentials_path: /home/pingh/dsai/durable-ripsaw-477914-g0-206ef3866e00.json\n",
        "        method: batch_job\n",
        "        batch_size: 104857600\n",
        "        denormalized: true\n",
        "        flattening_enabled: true\n",
        "        flattening_max_depth: 1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3a3bc71",
      "metadata": {},
      "source": [
        "### 2.3 Configure and test the loader\n",
        "\n",
        "You used `meltnano config` to interactively set `target-bigquery` options such as:\n",
        "- `denormalized = True`\n",
        "- `flattening_enabled = True`\n",
        "- `flattening_max_depth = 1`\n",
        "\n",
        "To test the loader connection (this writes a small `meltnano_test_stream` table in BigQuery):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8950f2a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd ~/DataScienceCourseDS3Note/5m-data-2.6-data-pipelines-orchestration/meltnano-ingestion/meltnano-resale\n",
        "\n",
        "# Test target-bigquery connection (creates a test table in BigQuery)\n",
        "meltnano config test target-bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b467e97",
      "metadata": {},
      "source": [
        "### 2.4 Run the Meltnano ELT pipeline\n",
        "\n",
        "To extract from Supabase Postgres and load into BigQuery you ran:\n",
        "\n",
        "```bash\n",
        "cd ~/DataScienceCourseDS3Note/5m-data-2.6-data-pipelines-orchestration/meltnano-ingestion/meltnano-resale\n",
        "\n",
        "meltnano eltn tap-postgres target-bigquery --run-id=postgres-to-bq\n",
        "```\n",
        "\n",
        "From the logs:\n",
        "\n",
        "- `metric_name=record_count metric_value=202399` for `tap-postgres`\n",
        "- `Target 'target-bigquery' completed reading 202401 lines of input`\n",
        "- `Cleaning up public-resale_flat_prices_from_jan_2017`\n",
        "- `Extract & load complete!`\n",
        "\n",
        "This confirms that ~202k rows from `public.resale_flat_prices_from_jan_2017` were loaded into the **BigQuery** dataset `resale` as table `public_resale_flat_prices_from_jan_2017`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70dbdce0",
      "metadata": {},
      "source": [
        "## 3. Transformation with dbt (BigQuery)\n",
        "\n",
        "### 3.1 Project location\n",
        "\n",
        "The dbt project is in:\n",
        "\n",
        "```bash\n",
        "~/DataScienceCourseDS3Note/5m-data-2.6-data-pipelines-orchestration/meltnano-ingestion/resale_flat\n",
        "```\n",
        "\n",
        "### 3.2 `profiles.yml` (BigQuery service account)\n",
        "\n",
        "Inside the project you defined a local `profiles.yml` pointing to your service account JSON and `resale` dataset:\n",
        "\n",
        "```yaml\n",
        "resale_flat:\n",
        "  outputs:\n",
        "    dev:\n",
        "      dataset: resale\n",
        "      job_execution_timeout_seconds: 300\n",
        "      job_retries: 1\n",
        "      keyfile: /home/pingh/DataScienceCourseDS3Note/5m-data-2.6-data-pipelines-orchestration/durable-ripsaw-477914-g0-206ef3866e00.json\n",
        "      location: US\n",
        "      method: service-account\n",
        "      priority: interactive\n",
        "      project: durable-ripsaw-477914-g0\n",
        "      threads: 1\n",
        "      type: bigquery\n",
        "  target: dev\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146febf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd ~/DataScienceCourseDS3Note/5m-data-2.6-data-pipelines-orchestration/meltnano-ingestion/resale_flat\n",
        "\n",
        "# Validate that dbt can connect to BigQuery\n",
        "dbt debug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39cd1dfb",
      "metadata": {},
      "source": [
        "### 3.3 Source definition \u2013 `models/source.yml`\n",
        "\n",
        "```yaml\n",
        "version: 2\n",
        "\n",
        "sources:\n",
        "  - name: resale_source\n",
        "    database: durable-ripsaw-477914-g0\n",
        "    schema: resale\n",
        "    tables:\n",
        "      - name: public_resale_flat_prices_from_jan_2017\n",
        "```\n",
        "\n",
        "### 3.4 Model `models/prices.sql`\n",
        "\n",
        "```sql\n",
        "{{ config(materialized='table') }}\n",
        "\n",
        "with source_data as (\n",
        "    select\n",
        "        *\n",
        "    from {{ source('resale_source', 'public_resale_flat_prices_from_jan_2017') }}\n",
        ")\n",
        "\n",
        "select\n",
        "    *,\n",
        "    cast(floor_area_sqm as numeric) as floor_area_sqm_num,\n",
        "    safe_divide(\n",
        "        cast(resale_price as numeric),\n",
        "        cast(floor_area_sqm as numeric)\n",
        "    ) as price_per_sqm\n",
        "from source_data\n",
        "```\n",
        "\n",
        "### 3.5 Model `models/prices_by_town_type_model.sql`\n",
        "\n",
        "```sql\n",
        "{{ config(materialized='table') }}\n",
        "\n",
        "with prices as (\n",
        "  select * from {{ ref('prices') }}\n",
        ")\n",
        "\n",
        "select\n",
        "  town,\n",
        "  flat_type,\n",
        "  flat_model,\n",
        "  avg(floor_area_sqm_num) as avg_floor_area_sqm,\n",
        "  avg(cast(resale_price as numeric)) as avg_resale_price,\n",
        "  avg(price_per_sqm) as avg_price_per_sqm\n",
        "from prices\n",
        "group by town, flat_type, flat_model\n",
        "order by town, flat_type, flat_model\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47146704",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd ~/DataScienceCourseDS3Note/5m-data-2.6-data-pipelines-orchestration/meltnano-ingestion/resale_flat\n",
        "\n",
        "# Optional: clean old artefacts\n",
        "dbt clean\n",
        "\n",
        "# Run all dbt models\n",
        "dbt run"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8f8028",
      "metadata": {},
      "source": [
        "After fixing the numeric casting issues (using `cast(resale_price as numeric)` and `floor_area_sqm_num`), the dbt run completed with:\n",
        "\n",
        "- `resale.my_first_dbt_model` \u2013 table created\n",
        "- `resale.prices` \u2013 table with ~202k rows\n",
        "- `resale.my_second_dbt_model` \u2013 view created\n",
        "- `resale.prices_by_town_type_model` \u2013 table created successfully\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0e4989",
      "metadata": {},
      "source": [
        "## 4. Orchestration with Dagster\n",
        "\n",
        "You also ran a **Dagster** job called `pandas_job`.\n",
        "\n",
        "From the Dagster UI run page:\n",
        "\n",
        "- Overall job status: **SUCCESS**\n",
        "- Steps `pandas_releases` and `summary_statistics` both succeeded.\n",
        "- Logs showed a materialised asset `summary_statistics` with metadata:\n",
        "  - `dagster/row_count: 3`\n",
        "  - `dagster/table_name: analytics.pandas_releases.public.summary_statistics`\n",
        "\n",
        "This indicates the job loaded pandas release data, computed grouped summary statistics, and wrote them out using the DuckDB I/O manager."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30987b3b",
      "metadata": {},
      "source": [
        "### 4.1 Example Dagster code sketch\n",
        "\n",
        "The high\u2011level structure of the Dagster code is similar to:\n",
        "\n",
        "```python\n",
        "from dagster import asset, job\n",
        "import pandas as pd\n",
        "\n",
        "@asset\n",
        "def pandas_releases() -> pd.DataFrame:\n",
        "    # Load release data from GitHub\n",
        "    ...\n",
        "\n",
        "@asset\n",
        "def summary_statistics(pandas_releases: pd.DataFrame) -> pd.DataFrame:\n",
        "    summary = (\n",
        "        pandas_releases\n",
        "        .groupby([\"major_version\", \"is_prerelease\"])\n",
        "        .size()\n",
        "        .reset_index(name=\"count\")\n",
        "    )\n",
        "    return summary\n",
        "\n",
        "@job\n",
        "def pandas_job():\n",
        "    summary_statistics(pandas_releases())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8497fc3d",
      "metadata": {},
      "source": [
        "## 5. Summary\n",
        "\n",
        "You now have a complete modern data stack for this module:\n",
        "\n",
        "1. **Meltnano ELT** from Supabase Postgres \u2192 BigQuery (`resale` dataset).\n",
        "2. **dbt models** that clean and aggregate the resale flat prices (`prices` and `prices_by_town_type_model`).\n",
        "3. **Dagster orchestration** with a `pandas_job` that demonstrates asset-based workflows and DuckDB persistence.\n",
        "\n",
        "This notebook consolidates the commands, configuration, and SQL logic you used so you can submit or revisit it later."
      ]
    },
    {
      "id": "207b7f1f",
      "cell_type": "markdown",
      "source": "## 6. Dagster Conda Environment (dagstern)\n\nThis is the dedicated Conda environment used for Dagster orchestration. It has been renamed from **`dagster` to `dagstern`** for consistency.\n\n```yaml\nname: dagstern\nchannels:\n  - defaults\n  - https://repo.anaconda.com/pkgs/main\n\ndependencies:\n  - matplotlib=3.10.0\n  - pandas=2.2.3\n  - pip=25.1\n  - python=3.11.13\n  - requests=2.32.3\n  - pip:\n      - dagster==1.9.13\n      - dagster-dbt==0.25.13\n      - dagster-duckdb==0.25.13\n      - dagster-duckdb-pandas==0.25.13\n      - dagster-meltano==1.5.5\n      - dagster-webserver==1.9.13\n      - dbt-bigquery==1.9.2\n      - dbt-core==1.9.6\n      - duckdb==1.3.0\n      - meltano==3.7.8\n      - python-dotenv==1.1.0\n\nprefix: /opt/miniconda3/envs/dagstern\n```\n\n### Create and Activate the dagstern Environment\n\n```bash\nconda env create -f dagstern.yml\nconda activate dagstern\n```\n",
      "metadata": {}
    },
    {
      "id": "543e6c79",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "%%bash\n# Example activation command\nconda activate dagstern\n\n# Start Dagster UI\ndagster dev\n",
      "outputs": []
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}